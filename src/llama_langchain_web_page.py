import os
import streamlit as st
import openai

from dotenv import load_dotenv
from streamlit_chat import message

from langchain import (
    GoogleSearchAPIWrapper,
    LLMChain
)
from langchain.memory import ConversationBufferMemory
from langchain.schema import (
    HumanMessage,
    AIMessage,
)
from langchain.agents import (
    load_tools,
    ZeroShotAgent,
    AgentExecutor
)
from langchain.chat_models import ChatOpenAI
from langchain.tools.base import (
    BaseTool,
)

from llama_hub.github_repo import GithubClient
from llama_index import (
    GPTVectorStoreIndex,
    LLMPredictor,
    ServiceContext,
    SimpleWebPageReader,
    StorageContext,
    load_index_from_storage,
)
from llama_index.storage.docstore import SimpleDocumentStore
from llama_index.storage.index_store import SimpleIndexStore
from llama_index.vector_stores import SimpleVectorStore

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
google_api_key = os.getenv("GOOGLE_API_KEY")
github_client = GithubClient(os.getenv("GITHUB_TOKEN"))

llm4 = ChatOpenAI(temperature=0.5, model_name="gpt-4")
llm3_5 = ChatOpenAI(temperature=0.1, model_name="gpt-3.5-turbo-16k")

search = GoogleSearchAPIWrapper(google_api_key = google_api_key)
index_directory = "./web_page_context"

# セッション内に保存されたチャット履歴のメモリの取得
try:
    memory = st.session_state["memory"]
except:
    memory = ConversationBufferMemory(return_messages=True)

# チャット履歴（HumanMessageやAIMessageなど）を格納する配列の初期化
history = []

class CustomSearchTool(BaseTool):
    name = "Search"
    description = "useful for when you need to answer questions about current events"

    def _run(self, query: str) -> str:
        """Use the tool."""
        return search.run(query)
    
    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("BingSearchRun does not support async")

def ensure_directory_exists(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)

def get_input() -> str:
    print("Insert your text. Enter 'q' or press Ctrl-D (or Ctrl-Z on Windows) to end.")
    contents = []
    while True:
        try:
            line = input()
        except EOFError:
            break
        if line == "q":
            break
        contents.append(line)
    return "\n".join(contents)

# 画面部分
st.title("LlamaIndex + LangChain + GPT4 AI for WebPage in Streamlit")
st.caption("by Marthur")

web_read_button = st.button("Web読み込み")
user_input = st.text_input("質問")
send_button = st.button("送信")

# GitHub読み込みボタン(LlamaIndex)押下処理
if web_read_button:
    web_read_button = False

    ensure_directory_exists(index_directory)

    llm_predictor = LLMPredictor(llm=llm3_5)
    service_context = ServiceContext.from_defaults(
        llm_predictor=llm_predictor
    )

    tools = [
        CustomSearchTool(),
        load_tools(["human"], input_func=get_input)[0]
    ]

    urls = [
        "https://github.com/MarthurGalarsy/SampleProduct/wiki",
        "https://github.com/MarthurGalarsy/SampleProduct/wiki/SamplePage"
    ]
    documents = SimpleWebPageReader(html_to_text=True).load_data(urls)
    if not os.path.exists(index_directory + "/web"):
        web_index = GPTVectorStoreIndex.from_documents(documents=documents, service_context=service_context)
        web_index.storage_context.persist(index_directory + "/web")
    else:
        storage_context_repo = StorageContext.from_defaults(
            docstore=SimpleDocumentStore.from_persist_dir(persist_dir=index_directory + "/web"),
            vector_store=SimpleVectorStore.from_persist_dir(persist_dir=index_directory + "/web"),
            index_store=SimpleIndexStore.from_persist_dir(persist_dir=index_directory + "/web"),
        )
        web_index = load_index_from_storage(storage_context_repo, service_context=service_context)
    web_query_engine = web_index.as_query_engine(service_context=service_context)

    class WebClass(BaseTool):
        name="SampleProduct Wiki"
        description = "ampleProduct Wikiについて情報の取得、表示するために使用します。"

        def _run(self, query: str) -> str:
            """Use the tool."""
            return web_query_engine.query(query).response
    
        async def _arun(self, query: str) -> str:
            """Use the tool asynchronously."""
            raise NotImplementedError("BingSearchRun does not support async")
    tools.append(WebClass())

    prompt = ZeroShotAgent.create_prompt(
        tools=tools,
        prefix="""あなたはGitHubやKotlinに詳しいシニアエンジニアです。
        次の質問にできる限り答えてください。次のツールにアクセスできます:""",
        suffix="""必ずFINAL FANTASY XIIIのライトニングの言葉遣いで回答してください。
        ただし、分からないことは人間に質問してください。
        質問内容：{question}
        {agent_scratchpad}
        """,
        input_variables=["question", "agent_scratchpad"]
    )
    llm_chain = LLMChain(llm=llm4, prompt=prompt)
    agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)

    agent_executor.save_agent(index_directory + "/agent.json")
    st.session_state["agent_executor"] = agent_executor

    if agent_executor: 
        memory.chat_memory.add_ai_message("読み込みました")

        # チャット履歴（HumanMessageやAIMessageなど）の読み込み
        try:
            history = memory.load_memory_variables({})["history"]
        except Exception as e:
            st.error(e)

if send_button :
    send_button = False
    agent_executor = st.session_state["agent_executor"]
    memory.chat_memory.add_user_message(user_input)

    response = agent_executor.run(user_input)

    memory.chat_memory.add_ai_message(response)
    st.session_state["memory"] = memory

    # チャット履歴（HumanMessageやAIMessageなど）の読み込み
    try:
        history = memory.load_memory_variables({})["history"]
    except Exception as e:
        st.error(e)

# チャット履歴の表示
for index, chat_message in enumerate(reversed(history)):
    if isinstance(chat_message, HumanMessage):
        message(chat_message.content, is_user=True, key=2 * index)
    elif isinstance(chat_message, AIMessage):
        message(chat_message.content, is_user=False, key=2 * index + 1)
